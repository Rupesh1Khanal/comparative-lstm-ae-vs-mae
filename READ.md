#  Comparative Study of LSTM-AE vs MAE for Anomaly Detection (NASA C-MAPSS FD001)

This repository contains the full pipeline, experiments, and results for a masterâ€™s thesis project comparing **LSTM Autoencoder (LSTM-AE)** and **Masked Autoencoder (MAE)** for anomaly detection on the **NASA C-MAPSS FD001 turbofan engine dataset**.  
The goal is to evaluate the effectiveness of **reconstruction-based** (LSTM-AE) vs **self-supervised masking-based** (MAE) anomaly detection methods.

---

## ğŸ“‚ Repository Structure

Comparison_of_models/
â”œâ”€â”€ artifacts/ # Generated artifacts: metrics, scores, PR arrays, weights
â”œâ”€â”€ data/ # Raw + processed NASA C-MAPSS FD001 data
â”œâ”€â”€ notebooks/ # Jupyter notebooks for preprocessing, training, visualization
â”œâ”€â”€ runs/ # Per-run experiment summaries (metrics, config, etc.)
â”œâ”€â”€ thesis_env/ # Local Python virtual environment (not versioned)
â”œâ”€â”€ .gitignore # Ignore large/binary artifacts + checkpoints
â”œâ”€â”€ config.yaml # Master config: preprocessing, training, evaluation settings
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt # Reproducible environment specification


### Key Directories

- **`artifacts/`**  
  Stores all derived outputs:
  - `preprocessing_meta.json`, `splits.json`, `yaml_hash.txt` (core reproducibility metadata)  
  - `metrics_*.json`, `scores_*.npy`, `pr_*.npy` (evaluation results)  
  - `*.h5` (trained model weights, checkpoints)  
  - `figures/` (plots generated by visualization notebook)

- **`data/`**  
  Contains raw and prepared versions of the NASA C-MAPSS FD001 dataset.

- **`notebooks/`**  
  - `preprocessing.ipynb` â†’ load, clean, window, scale, split dataset.  
  - `lstm_ae.ipynb` â†’ train & evaluate LSTM Autoencoder pipeline.  
  - `mae.ipynb` â†’ train & evaluate Masked Autoencoder pipeline.  
  - `visualization.ipynb` â†’ generate plots (PR curves, label histograms, etc.).

- **`runs/`**  
  Auto-created folders per experiment run with timestamp + config hash, e.g.:


---

## âš™ï¸ Installation

1. **Create and activate a virtual environment** (example with `venv`):

```bash
python -m venv thesis_env
# Linux/Mac
source thesis_env/bin/activate
# Windows
thesis_env\Scripts\activate
```


2. **Install dependencies form requirements.txt** :
```bash
pip install -r requirements.txt
```

## ğŸ“¦ Project Environment

This project was developed with:

Python 3.10

TensorFlow 2.10.1 (GPU-enabled, CUDA 11.2 + cuDNN 8)

NumPy

Pandas

scikit-learn

Matplotlib

PyYAML

## â–¶ï¸ Usage

Preprocessing
Run notebooks/preprocessing.ipynb
â†’ generates normalized sequences, splits, and metadata in artifacts/.

Train LSTM-AE
Run notebooks/lstm_ae.ipynb
â†’ saves weights (lstm_ae_best.weights.h5), validation/test metrics, PR curves.

Train MAE
Run notebooks/mae.ipynb
â†’ trains BiLSTM-based MAE with random masking; saves metrics and scores.

Visualization
Run notebooks/visualization.ipynb
â†’ generates comparative PR curves and class imbalance histograms into artifacts/figures/.

## ğŸ“Š Results (FD001)

Validation (best F1 threshold)

LSTM-AE: PR-AUC â‰ˆ 0.84, F1 â‰ˆ 0.74

MAE: PR-AUC â‰ˆ 0.61, F1 â‰ˆ 0.52

Test (raw vs warm-up calibration)

LSTM-AE: PR-AUC (raw) â‰ˆ 0.41, PR-AUC (warm-up) â‰ˆ 0.54

MAE: PR-AUC (raw) â‰ˆ 0.11, PR-AUC (warm-up) â‰ˆ 0.19

ğŸ‘‰ LSTM-AE consistently outperforms MAE across validation and test.
ğŸ‘‰ MAE shows limited robustness despite masking, indicating higher sensitivity to masking ratio and training stability.

ğŸ”„ Reproducibility

All hyperparameters are in config.yaml (sequence length, stride, enc/dec sizes, dropout, learning rate, mask ratios, evaluation thresholds).

Every run stores:

yaml_hash.txt â†’ MD5 fingerprint of config used at preprocessing.

env_snapshot.json â†’ Python + package versions.

run_summary.json â†’ metrics & thresholds for that run.

To fully reproduce:
```bash
python -m venv thesis_env
pip install -r requirements.txt
# Run preprocessing, then lstm_ae.ipynb / mae.ipynb
```

ğŸ“œ Citation

If you use this repository in academic work, please cite:

@mastersthesis{thesis2025,
  author       = {Rupesh Khanal},
  title        = {Comparative Study of Masked Autoencoders (Self-supervised Learning) and LSTM Autoencoder Baseline for Anomaly Detection in the NASA Turbofan Engine Time-series Dataset},
  school       = {GISMA University of Applied Sciences},
  year         = {2025},
  address      = {Berlin, Germany}
}

ğŸ™ Acknowledgements

NASA Ames Research Center for providing the C-MAPSS turbofan engine degradation dataset.

Kaggle for hosting the dataset version used in this project.

Open-source libraries: TensorFlow, NumPy, scikit-learn, Matplotlib, PyYAML.
