{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6137bfa",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder — Training & Evaluation (FD001)\n",
    "\n",
    "Baseline reconstruction model:\n",
    "- stacked LSTM encoder–decoder (128→64→latent=32 → decoder mirrored),\n",
    "- loss: MSE, variance-normalized scoring,\n",
    "- validation percentile sweep for threshold (F1-optimal),\n",
    "- warm-up calibration on test (K=50), run-length filter=3,\n",
    "- saves metrics, scores, and PR curve points to `artifacts/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca93af3",
   "metadata": {},
   "source": [
    "## 1) Load Config & Set Seeds\n",
    "- `config.yaml` -> `training_lstm`, `evaluation`\n",
    "- deterministic TF ops (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ae728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, yaml, hashlib, time, random, shutil, sys, subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, callbacks, optimizers\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score, f1_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "T = CFG[\"training_lstm\"]\n",
    "E = CFG[\"evaluation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bd3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42, tf_deterministic=True):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)\n",
    "    if tf_deterministic:\n",
    "        os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "        try: tf.config.experimental.enable_op_determinism(True)\n",
    "        except: pass\n",
    "set_seeds(CFG[\"repro\"][\"seed\"], CFG[\"repro\"].get(\"tf_deterministic\", True))\n",
    "\n",
    "# Postproc helpers\n",
    "def run_length_filter(pred_bin, min_run=1):\n",
    "    if min_run <= 1: return pred_bin.astype(int)\n",
    "    y = pred_bin.astype(int).copy()\n",
    "    i = 0\n",
    "    while i < len(y):\n",
    "        if y[i] == 1:\n",
    "            j = i\n",
    "            while j < len(y) and y[j] == 1: j += 1\n",
    "            if (j - i) < min_run: y[i:j] = 0\n",
    "            i = j\n",
    "        else: i += 1\n",
    "    return y\n",
    "\n",
    "def summarize_metrics(y_true, y_pred, scores):\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    pr_auc = average_precision_score(y_true, scores)\n",
    "    return {\"precision\": float(prec), \"recall\": float(rec),\n",
    "            \"f1\": float(f1), \"pr_auc\": float(pr_auc)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb8bf5",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load Arrays & Sanity Checks\n",
    " - Uses preprocessed arrays saved by the preprocessing pipeline\n",
    " - Confirms `config.yaml` hash matches preprocessing (or warns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871b8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shapes] Xtr=(7487, 80, 15)  Xva=(2524, 80, 15)  yva=(2524,)\n"
     ]
    }
   ],
   "source": [
    "Xtr = np.load(\"artifacts/X_train_normal.npy\")\n",
    "Xva = np.load(\"artifacts/X_val.npy\")\n",
    "yva = np.load(\"artifacts/y_val.npy\").astype(int)\n",
    "META = json.load(open(\"artifacts/preprocessing_meta.json\",\"r\",encoding=\"utf-8\"))\n",
    "\n",
    "L, D = Xtr.shape[1], Xtr.shape[2]\n",
    "print(f\"[shapes] Xtr={Xtr.shape}  Xva={Xva.shape}  yva={yva.shape}\")\n",
    "\n",
    "# Sanity: YAML drift guard\n",
    "def yaml_md5(path=\"config.yaml\"):\n",
    "    with open(path, \"rb\") as f: return hashlib.md5(f.read()).hexdigest()\n",
    "if META[\"yaml_hash\"] != yaml_md5(\"config.yaml\"):\n",
    "    print(\"WARNING: YAML drift detected (config.yaml changed since preprocessing).\")\n",
    "    print(\"Proceeding because seq_len/stride/scaling must match.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744b2f5",
   "metadata": {},
   "source": [
    "## 3) Build Model — LSTM Autoencoder\n",
    "\n",
    "**Architecture**\n",
    "- Stacked LSTM encoder–decoder:\n",
    "  - Encoder: LSTM(enc1) → LSTM(enc2) → LSTM(latent)\n",
    "  - Decoder: RepeatVector(seq_len) → LSTM(enc2) → LSTM(enc1) → TimeDistributed(Dense(n_features))\n",
    "- Dropout applied after recurrent layers.\n",
    "- Hyperparameters read from `config.yaml` → `training_lstm` (enc1, enc2, latent, dropout, loss, lr).\n",
    "\n",
    "**Inputs & shapes**\n",
    "- Sequence length = `seq_len` from preprocessing (e.g., 80)\n",
    "- Features = number of selected sensors after preprocessing\n",
    "\n",
    "**What this cell does**\n",
    "- Builds the Keras model for the current shapes\n",
    "- Compiles with MSE  in YAML\n",
    "- Prints a model summary for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fdfdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_ae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 80, 15)]          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 80, 128)           73728     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 80, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 80, 64)            49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 80, 64)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 80, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 80, 64)            24832     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 80, 64)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 80, 128)           98816     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 80, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 80, 15)           1935      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,135\n",
      "Trainable params: 261,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_lstm_ae(seq_len, n_feats, enc1=128, enc2=64, latent=32, dropout=0.2):\n",
    "    In = layers.Input(shape=(seq_len, n_feats))\n",
    "    x = layers.LSTM(enc1, return_sequences=True)(In); x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LSTM(enc2, return_sequences=True)(x); x = layers.Dropout(dropout)(x)\n",
    "    z = layers.LSTM(latent)(x); z = layers.Dropout(dropout)(z)\n",
    "    y = layers.RepeatVector(seq_len)(z)\n",
    "    y = layers.LSTM(enc2, return_sequences=True)(y); y = layers.Dropout(dropout)(y)\n",
    "    y = layers.LSTM(enc1, return_sequences=True)(y); y = layers.Dropout(dropout)(y)\n",
    "    Out = layers.TimeDistributed(layers.Dense(n_feats))(y)\n",
    "    return models.Model(In, Out, name=\"lstm_ae\")\n",
    "\n",
    "model = build_lstm_ae(L, D, T[\"enc1\"], T[\"enc2\"], T[\"latent\"], T.get(\"dropout\",0.2))\n",
    "loss = \"mse\" if T.get(\"loss\",\"mse\").lower()==\"mse\" else keras.losses.Huber(delta=float(T.get(\"huber_delta\",1.0)))\n",
    "opt = keras.optimizers.Adam(learning_rate=T[\"lr\"])\n",
    "model.compile(optimizer=opt, loss=loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b031f8",
   "metadata": {},
   "source": [
    "## 4) Train — Callbacks & Checkpoints\n",
    " - Train on normal windows only\n",
    "- Validation = normal windows from X_val (if present), else `validation_split`\n",
    " - Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    " - Saves: `lstm_ae_best.weights.h5`, `lstm_ae_final.weights.h5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d50f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Using X_val normals for validation: n=1844\n",
      "Epoch 1/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.5316\n",
      "Epoch 1: val_loss improved from inf to 0.45591, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 17s 56ms/step - loss: 0.5316 - val_loss: 0.4559 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4564\n",
      "Epoch 2: val_loss improved from 0.45591 to 0.43686, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4562 - val_loss: 0.4369 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4417\n",
      "Epoch 3: val_loss improved from 0.43686 to 0.42628, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4416 - val_loss: 0.4263 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4353\n",
      "Epoch 4: val_loss improved from 0.42628 to 0.42401, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 6s 48ms/step - loss: 0.4351 - val_loss: 0.4240 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4321\n",
      "Epoch 5: val_loss improved from 0.42401 to 0.42265, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4321 - val_loss: 0.4227 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4289\n",
      "Epoch 6: val_loss improved from 0.42265 to 0.42000, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 47ms/step - loss: 0.4289 - val_loss: 0.4200 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4273\n",
      "Epoch 7: val_loss improved from 0.42000 to 0.41970, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4273 - val_loss: 0.4197 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4255\n",
      "Epoch 8: val_loss did not improve from 0.41970\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4255 - val_loss: 0.4208 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4238\n",
      "Epoch 9: val_loss improved from 0.41970 to 0.41966, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4237 - val_loss: 0.4197 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4228\n",
      "Epoch 10: val_loss improved from 0.41966 to 0.41845, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4228 - val_loss: 0.4185 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4216\n",
      "Epoch 11: val_loss improved from 0.41845 to 0.41817, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4216 - val_loss: 0.4182 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4210\n",
      "Epoch 12: val_loss did not improve from 0.41817\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4210 - val_loss: 0.4214 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4210\n",
      "Epoch 13: val_loss improved from 0.41817 to 0.41686, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4210 - val_loss: 0.4169 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4199\n",
      "Epoch 14: val_loss improved from 0.41686 to 0.41592, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4199 - val_loss: 0.4159 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4191\n",
      "Epoch 15: val_loss did not improve from 0.41592\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4191 - val_loss: 0.4164 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4183\n",
      "Epoch 16: val_loss did not improve from 0.41592\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4185 - val_loss: 0.4160 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4179\n",
      "Epoch 17: val_loss did not improve from 0.41592\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4179 - val_loss: 0.4209 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4178\n",
      "Epoch 18: val_loss did not improve from 0.41592\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4178 - val_loss: 0.4164 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4172\n",
      "Epoch 19: val_loss improved from 0.41592 to 0.41538, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4172 - val_loss: 0.4154 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4165\n",
      "Epoch 20: val_loss improved from 0.41538 to 0.41365, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4165 - val_loss: 0.4137 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4165\n",
      "Epoch 21: val_loss did not improve from 0.41365\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4165 - val_loss: 0.4156 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4159\n",
      "Epoch 22: val_loss did not improve from 0.41365\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4159 - val_loss: 0.4147 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4156\n",
      "Epoch 23: val_loss did not improve from 0.41365\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4156 - val_loss: 0.4143 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4153\n",
      "Epoch 24: val_loss improved from 0.41365 to 0.41322, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4153 - val_loss: 0.4132 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4146\n",
      "Epoch 25: val_loss did not improve from 0.41322\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4146 - val_loss: 0.4133 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4143\n",
      "Epoch 26: val_loss did not improve from 0.41322\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4142 - val_loss: 0.4157 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4139\n",
      "Epoch 27: val_loss improved from 0.41322 to 0.41214, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4139 - val_loss: 0.4121 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4139\n",
      "Epoch 28: val_loss did not improve from 0.41214\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4138 - val_loss: 0.4130 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4137\n",
      "Epoch 29: val_loss did not improve from 0.41214\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4137 - val_loss: 0.4131 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4134\n",
      "Epoch 30: val_loss did not improve from 0.41214\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4134 - val_loss: 0.4130 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4130\n",
      "Epoch 31: val_loss did not improve from 0.41214\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4131 - val_loss: 0.4151 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4130\n",
      "Epoch 32: val_loss improved from 0.41214 to 0.41113, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4130 - val_loss: 0.4111 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4128\n",
      "Epoch 33: val_loss did not improve from 0.41113\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4128 - val_loss: 0.4133 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4125\n",
      "Epoch 34: val_loss did not improve from 0.41113\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4126 - val_loss: 0.4137 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4127\n",
      "Epoch 35: val_loss did not improve from 0.41113\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4126 - val_loss: 0.4140 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4124\n",
      "Epoch 36: val_loss did not improve from 0.41113\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4123 - val_loss: 0.4131 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4123\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.41113\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4123 - val_loss: 0.4124 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4114\n",
      "Epoch 38: val_loss did not improve from 0.41113\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4114 - val_loss: 0.4119 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4112\n",
      "Epoch 39: val_loss did not improve from 0.41113\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4112 - val_loss: 0.4113 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4111\n",
      "Epoch 40: val_loss improved from 0.41113 to 0.41048, saving model to artifacts\\lstm_ae_best.weights.h5\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4111 - val_loss: 0.4105 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4110\n",
      "Epoch 41: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4110 - val_loss: 0.4129 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4111\n",
      "Epoch 42: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4111 - val_loss: 0.4110 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4109\n",
      "Epoch 43: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4109 - val_loss: 0.4119 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4111\n",
      "Epoch 44: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 44ms/step - loss: 0.4111 - val_loss: 0.4110 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4108\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4108 - val_loss: 0.4112 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4105\n",
      "Epoch 46: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4105 - val_loss: 0.4108 - lr: 1.2500e-04\n",
      "Epoch 47/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4106\n",
      "Epoch 47: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4106 - val_loss: 0.4112 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4106\n",
      "Epoch 48: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4105 - val_loss: 0.4117 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.4104\n",
      "Epoch 49: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 45ms/step - loss: 0.4105 - val_loss: 0.4110 - lr: 1.2500e-04\n",
      "Epoch 50/100\n",
      "117/117 [==============================] - ETA: 0s - loss: 0.4103Restoring model weights from the end of the best epoch: 40.\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.41048\n",
      "117/117 [==============================] - 5s 46ms/step - loss: 0.4103 - val_loss: 0.4112 - lr: 1.2500e-04\n",
      "Epoch 50: early stopping\n"
     ]
    }
   ],
   "source": [
    "ART = \"artifacts\"; os.makedirs(ART, exist_ok=True)\n",
    "ckpt_best = os.path.join(ART, \"lstm_ae_best.weights.h5\")\n",
    "\n",
    "\n",
    "Xva_norm = Xva[yva == 0]\n",
    "use_val_split = len(Xva_norm) == 0  \n",
    "\n",
    "es  = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=int(T.get(\"early_stopping_patience\", 10)),\n",
    "    min_delta=float(T.get(\"early_stopping_min_delta\", 1e-4)),\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "rlr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=int(T.get(\"rlr_patience\", 5)),\n",
    "    min_delta=float(T.get(\"early_stopping_min_delta\", 1e-4)),\n",
    "    cooldown=2,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1,\n",
    ")\n",
    "mc  = callbacks.ModelCheckpoint(\n",
    "    ckpt_best, monitor=\"val_loss\", save_best_only=True, save_weights_only=True, verbose=1\n",
    ")\n",
    "\n",
    "if use_val_split:\n",
    "    print(\"[train] No normal windows in X_val → using validation_split from Xtr.\")\n",
    "    hist = model.fit(\n",
    "        Xtr, Xtr,\n",
    "        validation_split=T.get(\"val_frac\", 0.10),\n",
    "        epochs=T[\"epochs\"],\n",
    "        batch_size=T[\"batch\"],\n",
    "        shuffle=True,\n",
    "        callbacks=[es, rlr, mc],\n",
    "        verbose=1,\n",
    "    )\n",
    "else:\n",
    "    print(f\"[train] Using X_val normals for validation: n={len(Xva_norm)}\")\n",
    "    hist = model.fit(\n",
    "        Xtr, Xtr,\n",
    "        validation_data=(Xva_norm, Xva_norm),\n",
    "        epochs=T[\"epochs\"],\n",
    "        batch_size=T[\"batch\"],\n",
    "        shuffle=True,\n",
    "        callbacks=[es, rlr, mc],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "model.save_weights(os.path.join(ART, \"lstm_ae_final.weights.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bab590",
   "metadata": {},
   "source": [
    "## 3) Validation Scoring & Threshold Sweep (LSTM-AE)\n",
    "\n",
    "- Compute variance-normalized mean squared error (per-feature variance from training residuals).  \n",
    "- Sweep thresholds across percentiles (70–99.9) to maximize F1 on the validation set.  \n",
    "- Save outputs:\n",
    "  - `feat_var_lstm.npy` — variance vector for normalization  \n",
    "  - `scores_lstm_val.npy` — anomaly scores for validation set  \n",
    "  - `sweep_lstm_val.npy` — sweep results (p, threshold, precision, recall, F1)  \n",
    "  - `metrics_lstm_val.json` — summary of chosen threshold and metrics  \n",
    "  - `pr_lstm_val.npy` — precision–recall curve data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad53962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] {'precision': 0.8282097649186256, 'recall': 0.6735294117647059, 'f1': 0.7429034874290349, 'pr_auc': 0.8375025879916973} thr=2.081047 pct=78.10\n",
      "Saved: feat_var_lstm.npy, scores_lstm_val.npy, sweep_lstm_val.npy, metrics_lstm_val.json, pr_lstm_val.npy\n"
     ]
    }
   ],
   "source": [
    "pred_tr = model.predict(Xtr, verbose=0)\n",
    "res_tr  = (pred_tr - Xtr)**2\n",
    "feat_var = np.clip(res_tr.reshape(-1, D).var(axis=0), 1e-8, None)\n",
    "\n",
    "pred_va = model.predict(Xva, verbose=0)\n",
    "res_va  = (pred_va - Xva)**2\n",
    "scores_va = (res_va / feat_var[None, None, :]).mean(axis=(1, 2))\n",
    "\n",
    "\n",
    "pmin = float(E.get(\"sweep\", {}).get(\"pmin\", 70.0))\n",
    "pmax = float(E.get(\"sweep\", {}).get(\"pmax\", 99.9))\n",
    "step = float(E.get(\"sweep\", {}).get(\"step\", 0.1))\n",
    "\n",
    "cands = np.arange(pmin, pmax + 1e-9, step, dtype=float)\n",
    "\n",
    "best = {\"f1\": -1.0}\n",
    "sweep_rows = []\n",
    "for p in cands:\n",
    "    thr = np.percentile(scores_va, p)\n",
    "    yhat = (scores_va >= thr).astype(int)\n",
    "    yhat = run_length_filter(yhat, int(E.get(\"min_run_length\", 1)))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(yva, yhat, average=\"binary\", zero_division=0)\n",
    "    pr_auc = average_precision_score(yva, scores_va)\n",
    "    sweep_rows.append([p, float(thr), float(prec), float(rec), float(f1)])\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"p\": float(p), \"thr\": float(thr), \"f1\": float(f1), \"yhat\": yhat}\n",
    "\n",
    "m_val = summarize_metrics(yva, best[\"yhat\"], scores_va)\n",
    "print(\"[VAL]\", m_val, f\"thr={best['thr']:.6f} pct={best['p']:.2f}\")\n",
    "\n",
    "\n",
    "np.save(os.path.join(ART, \"feat_var_lstm.npy\"), feat_var)\n",
    "np.save(os.path.join(ART, \"scores_lstm_val.npy\"), scores_va)\n",
    "np.save(os.path.join(ART, \"sweep_lstm_val.npy\"), np.array(sweep_rows, dtype=float))  # columns: p,thr,prec,rec,f1\n",
    "\n",
    "val_payload = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"metrics\": m_val,\n",
    "    \"threshold_mode\": \"val_percentile_sweep_varnorm\",\n",
    "    \"val_percentile\": best[\"p\"],\n",
    "    \"threshold_value\": best[\"thr\"],\n",
    "    \"postproc\": {\"min_run_length\": int(E.get(\"min_run_length\", 1))},\n",
    "    \"scoring\": {\"type\": \"variance_normalized_mean\"},\n",
    "    \"sweep\": {\"pmin\": pmin, \"pmax\": pmax, \"step\": step},\n",
    "}\n",
    "with open(os.path.join(ART, \"metrics_lstm_val.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_payload, f, indent=2)\n",
    "\n",
    "\n",
    "prec_va, rec_va, thr_va = precision_recall_curve(yva, scores_va)\n",
    "np.save(os.path.join(ART, \"pr_lstm_val.npy\"), np.c_[rec_va[:-1], prec_va[:-1], thr_va])\n",
    "\n",
    "print(\"Saved: feat_var_lstm.npy, scores_lstm_val.npy, sweep_lstm_val.npy, metrics_lstm_val.json, pr_lstm_val.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb89c9",
   "metadata": {},
   "source": [
    "## 6) Test Scoring, Warm-Up Calibration, PR Curves\n",
    " - Compute raw test scores\n",
    " - Warm-up z-score per engine (first K=50 windows), then fixed threshold from validation\n",
    " - Saves: `scores_lstm_test.npy`, `scores_lstm_test_varnorm_warmup.npy`,\n",
    "          `pr_lstm_test.npy`, `pr_lstm_test_warmup.npy`, `metrics_lstm_test.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b626c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Xte=(5660, 80, 15) positives=407\n",
      "[TEST] PR-AUC raw(varnorm)=0.415\n"
     ]
    }
   ],
   "source": [
    "ART = \"artifacts\"\n",
    "\n",
    "def build_lstm_ae(seq_len, n_feats, Tconf):\n",
    "    In = layers.Input(shape=(seq_len, n_feats))\n",
    "    x = layers.LSTM(Tconf[\"enc1\"], return_sequences=True)(In); x = layers.Dropout(Tconf.get(\"dropout\",0.2))(x)\n",
    "    x = layers.LSTM(Tconf[\"enc2\"], return_sequences=True)(x);  x = layers.Dropout(Tconf.get(\"dropout\",0.2))(x)\n",
    "    z = layers.LSTM(Tconf[\"latent\"])(x);                        z = layers.Dropout(Tconf.get(\"dropout\",0.2))(z)\n",
    "    y = layers.RepeatVector(seq_len)(z)\n",
    "    y = layers.LSTM(Tconf[\"enc2\"], return_sequences=True)(y);   y = layers.Dropout(Tconf.get(\"dropout\",0.2))(y)\n",
    "    y = layers.LSTM(Tconf[\"enc1\"], return_sequences=True)(y);   y = layers.Dropout(Tconf.get(\"dropout\",0.2))(y)\n",
    "    Out = layers.TimeDistributed(layers.Dense(n_feats))(y)\n",
    "    return models.Model(In, Out, name=\"lstm_ae\")\n",
    "\n",
    "with open(\"config.yaml\",\"r\",encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "T = CFG[\"training_lstm\"]\n",
    "\n",
    "# load TEST arrays\n",
    "Xte = np.load(os.path.join(ART,\"X_test.npy\"))\n",
    "yte = np.load(os.path.join(ART,\"y_test.npy\")).astype(int)\n",
    "L, D = Xte.shape[1], Xte.shape[2]\n",
    "print(f\"[TEST] Xte={Xte.shape} positives={int(yte.sum())}\")\n",
    "\n",
    "# rebuild & load weights\n",
    "model_te = build_lstm_ae(L, D, T)\n",
    "model_te.load_weights(os.path.join(ART,\"lstm_ae_best.weights.h5\"))\n",
    "\n",
    "# load VAL variance basis\n",
    "feat_var = np.load(os.path.join(ART,\"feat_var_lstm.npy\")).astype(np.float32)\n",
    "denom = np.clip(feat_var, 1e-8, None)[None,None,:]\n",
    "\n",
    "# compute raw scores\n",
    "def score_vnorm_mean_mse(model, X, denom, bs=128):\n",
    "    scores = np.zeros(len(X), dtype=np.float32)\n",
    "    for i in range(0,len(X),bs):\n",
    "        xb = X[i:i+bs].astype(np.float32, copy=False)\n",
    "        xh = model.predict_on_batch(xb)\n",
    "        resid2 = (xb - xh)**2 / denom\n",
    "        scores[i:i+bs] = resid2.mean(axis=(1,2))\n",
    "    return scores\n",
    "\n",
    "s_te_raw = score_vnorm_mean_mse(model_te, Xte, denom, bs=128)\n",
    "np.save(os.path.join(ART,\"scores_lstm_test.npy\"), s_te_raw)\n",
    "\n",
    "pr_auc_raw = float(average_precision_score(yte, s_te_raw))\n",
    "prec_raw, rec_raw, thr_raw = precision_recall_curve(yte, s_te_raw)\n",
    "pr_curve_raw = np.c_[rec_raw[:-1], prec_raw[:-1], thr_raw]\n",
    "np.save(os.path.join(ART,\"pr_lstm_test.npy\"), pr_curve_raw)\n",
    "\n",
    "print(f\"[TEST] PR-AUC raw(varnorm)={pr_auc_raw:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17259479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] PR-AUC warmup(varnorm)=0.537 (K=50)\n",
      "[TEST|varnorm+warmup] F1=0.436 P=0.361 R=0.550 PR-AUC=0.537 pct≈70.20\n"
     ]
    }
   ],
   "source": [
    "ART = \"artifacts\"\n",
    "with open(\"config.yaml\",\"r\",encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "E = CFG[\"evaluation\"]\n",
    "\n",
    "\n",
    "warm_cfg = E.get(\"calibration\",{}).get(\"warmup\",{})\n",
    "WARM_EN = bool(warm_cfg.get(\"enable\",True))\n",
    "K_WARM = int(warm_cfg.get(\"K\",50))\n",
    "\n",
    "\n",
    "s_te_raw = np.load(os.path.join(ART,\"scores_lstm_test.npy\"))\n",
    "yte      = np.load(os.path.join(ART,\"y_test.npy\")).astype(int)\n",
    "ids_val_path, ids_test_path = os.path.join(ART,\"unit_ids_val.npy\"), os.path.join(ART,\"unit_ids_test.npy\")\n",
    "s_val_path = os.path.join(ART,\"scores_lstm_val.npy\")\n",
    "have_ids = all(os.path.exists(p) for p in [ids_val_path, ids_test_path, s_val_path])\n",
    "\n",
    "def warmup_zscore(scores, unit_ids, K=50):\n",
    "    scores = scores.astype(np.float32, copy=True)\n",
    "    out = scores.copy()\n",
    "    for uid in np.unique(unit_ids):\n",
    "        idx = np.where(unit_ids==uid)[0]\n",
    "        base = scores[idx[:min(K,len(idx))]]\n",
    "        mu, sd = float(base.mean()), float(base.std())\n",
    "        if sd < 1e-8: sd=1.0\n",
    "        out[idx] = (scores[idx]-mu)/sd\n",
    "    return out\n",
    "\n",
    "if WARM_EN and have_ids:\n",
    "    ids_val = np.load(ids_val_path); ids_test = np.load(ids_test_path)\n",
    "    s_val_raw = np.load(s_val_path)\n",
    "    s_val_adj = warmup_zscore(s_val_raw, ids_val, K=K_WARM)\n",
    "    s_te_adj  = warmup_zscore(s_te_raw,  ids_test, K=K_WARM)\n",
    "\n",
    "    pr_auc_warm = float(average_precision_score(yte, s_te_adj))\n",
    "    print(f\"[TEST] PR-AUC warmup(varnorm)={pr_auc_warm:.3f} (K={K_WARM})\")\n",
    "\n",
    "    \n",
    "    yva = np.load(os.path.join(ART,\"y_val.npy\")).astype(int)\n",
    "    pmin,pmax,step = E[\"sweep\"][\"pmin\"],E[\"sweep\"][\"pmax\"],E[\"sweep\"][\"step\"]\n",
    "    best = {\"f1\":-1}\n",
    "    for p in np.arange(pmin,pmax+1e-9,step):\n",
    "        thr = np.percentile(s_val_adj,p)\n",
    "        yhat = (s_val_adj>=thr).astype(int)\n",
    "        f1 = f1_score(yva,yhat,zero_division=0)\n",
    "        if f1>best[\"f1\"]: best={\"p\":float(p),\"thr\":float(thr),\"f1\":float(f1)}\n",
    "\n",
    "   \n",
    "    yhat_te = (s_te_adj>=best[\"thr\"]).astype(int)\n",
    "    prec,rec,f1,_ = precision_recall_fscore_support(yte,yhat_te,average=\"binary\",zero_division=0)\n",
    "\n",
    "    \n",
    "    np.save(os.path.join(ART,\"scores_lstm_test_varnorm_warmup.npy\"), s_te_adj)\n",
    "    prec_w, rec_w, thr_w = precision_recall_curve(yte,s_te_adj)\n",
    "    pr_curve_warm = np.c_[rec_w[:-1], prec_w[:-1], thr_w]\n",
    "    np.save(os.path.join(ART,\"pr_lstm_test_warmup.npy\"), pr_curve_warm)\n",
    "\n",
    "    \n",
    "    val_payload = json.load(open(os.path.join(ART,\"metrics_lstm_val.json\")))\n",
    "    metrics_test = {\n",
    "        \"pr_auc_raw\": float(average_precision_score(yte,s_te_raw)),\n",
    "        \"pr_auc_warmup\": pr_auc_warm,\n",
    "        \"calibration\": {\"warmup\":{\"enabled\":True,\"K\":K_WARM}},\n",
    "        \"val_percentile\": best[\"p\"], \"threshold_value\": best[\"thr\"],\n",
    "        \"confusion_at_chosen\":{\n",
    "            \"precision\": float(prec),\"recall\":float(rec),\"f1\":float(f1),\n",
    "            \"tp\": int(((yhat_te==1)&(yte==1)).sum()),\n",
    "            \"fp\": int(((yhat_te==1)&(yte==0)).sum()),\n",
    "            \"fn\": int(((yhat_te==0)&(yte==1)).sum()),\n",
    "            \"tn\": int(((yhat_te==0)&(yte==0)).sum()),\n",
    "            \"n_windows\": int(len(yte)),\"n_pos\":int(yte.sum())\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(ART,\"metrics_lstm_test.json\"),\"w\") as f: json.dump(metrics_test,f,indent=2)\n",
    "    print(f\"[TEST|varnorm+warmup] F1={f1:.3f} P={prec:.3f} R={rec:.3f} PR-AUC={pr_auc_warm:.3f} pct≈{best['p']:.2f}\")\n",
    "else:\n",
    "    print(\"[INFO] Warm-up disabled or missing ids — only raw metrics kept.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeae7c1",
   "metadata": {},
   "source": [
    "## 7) Run Summary, Save PR curves & Environment Snapshot\n",
    " - Save compact run summary under `runs/`\n",
    " - Save library versions to `artifacts/env_snapshot.json` for reproducibility\n",
    " - Validation PR, Test PR (raw), Test PR (warm-up)\n",
    "- (Figures can always be regenerated from the saved `.npy` curve points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f88efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL PR-AUC:   0.8375025879916973\n",
      "TEST PR-AUC (raw):   0.415\n",
      "TEST PR-AUC (warmup): 0.537  <-- best\n",
      "Run folder: runs\\2025-09-21_1456_cfg_2cff7a\n"
     ]
    }
   ],
   "source": [
    "ART = Path(\"artifacts\"); RUNS = Path(\"runs\"); RUNS.mkdir(exist_ok=True)\n",
    "\n",
    "def safe_read_json(p, default=None):\n",
    "    try:\n",
    "        return json.load(open(p, \"r\", encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return default if default is not None else {}\n",
    "\n",
    "yaml_hash = (ART/\"yaml_hash.txt\").read_text().strip() if (ART/\"yaml_hash.txt\").exists() else \"unknown\"\n",
    "meta       = safe_read_json(ART/\"preprocessing_meta.json\", {})\n",
    "val_payload= safe_read_json(ART/\"metrics_lstm_val.json\", {})\n",
    "test_payload= safe_read_json(ART/\"metrics_lstm_test.json\", {})\n",
    "\n",
    "pr_auc_raw  = float(test_payload.get(\"pr_auc_raw\", test_payload.get(\"pr_auc\", float(\"nan\"))))\n",
    "pr_auc_warm = test_payload.get(\"pr_auc_warmup\", None)\n",
    "use_warmup  = (pr_auc_warm is not None) and (not np.isnan(pr_auc_warm)) and (pr_auc_warm >= pr_auc_raw)\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"yaml_hash\": yaml_hash,\n",
    "    \"data\": {\n",
    "        \"dataset\": \"FD001\",\n",
    "        \"seq_len\": meta.get(\"seq_len\"),\n",
    "        \"stride\": meta.get(\"stride\"),\n",
    "        \"n_features\": meta.get(\"n_features\"),\n",
    "        \"scale\": meta.get(\"scale\"),\n",
    "        \"scale_fit_on\": meta.get(\"scale_fit_on\"),\n",
    "        \"label_mode\": meta.get(\"label_mode\"),\n",
    "        \"label_dilation\": meta.get(\"label_dilation\"),\n",
    "        \"anomaly_horizon\": meta.get(\"anomaly_horizon\"),\n",
    "        \"dropped_fixed\": meta.get(\"dropped_fixed\"),\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"percentile\": val_payload.get(\"val_percentile\"),\n",
    "        \"threshold\": val_payload.get(\"threshold_value\"),\n",
    "        \"metrics\": val_payload.get(\"metrics\", {}),\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"pr_auc_raw\": pr_auc_raw,\n",
    "        \"pr_auc_warmup\": None if not use_warmup else float(pr_auc_warm),\n",
    "        \"used_calibration\": \"warmup_zscore\" if use_warmup else \"none\",\n",
    "        \"metrics_payload\": test_payload,\n",
    "    },\n",
    "}\n",
    "\n",
    "rid = time.strftime(\"%Y-%m-%d_%H%M\") + f\"_cfg_{str(yaml_hash)[:6]}\"\n",
    "rdir = RUNS / rid\n",
    "rdir.mkdir(parents=True, exist_ok=True)\n",
    "with open(rdir/\"run_summary.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "with open(ART/\"LATEST_RUN.txt\",\"w\") as f:\n",
    "    f.write(str(rdir) + \"\\n\")\n",
    "\n",
    "print(f\"VAL PR-AUC:   {summary['validation']['metrics'].get('pr_auc', 'n/a')}\")\n",
    "print(f\"TEST PR-AUC (raw):   {pr_auc_raw:.3f}\")\n",
    "print(\"TEST PR-AUC (warmup):\", \"n/a\" if not use_warmup else f\"{pr_auc_warm:.3f}  <-- best\")\n",
    "print(\"Run folder:\", rdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1c527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PR curves (if available) to artifacts/.\n"
     ]
    }
   ],
   "source": [
    "ART = \"artifacts\"\n",
    "\n",
    "def save_pr(np_file, out_png, title):\n",
    "    arr = np.load(os.path.join(ART, np_file))\n",
    "    # arr columns: [recall, precision, threshold]\n",
    "    recall, precision = arr[:,0], arr[:,1]\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, linewidth=2)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(title)\n",
    "    plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ART, out_png), dpi=150); plt.close()\n",
    "\n",
    "if os.path.exists(os.path.join(ART,\"pr_lstm_val.npy\")):\n",
    "    save_pr(\"pr_lstm_val.npy\", \"pr_lstm_val.png\", \"PR Curve — Validation\")\n",
    "\n",
    "if os.path.exists(os.path.join(ART,\"pr_lstm_test.npy\")):\n",
    "    save_pr(\"pr_lstm_test.npy\", \"pr_lstm_test.png\", \"PR Curve — Test (raw)\")\n",
    "\n",
    "if os.path.exists(os.path.join(ART,\"pr_lstm_test_warmup.npy\")):\n",
    "    save_pr(\"pr_lstm_test_warmup.npy\", \"pr_lstm_test_warmup.png\", \"PR Curve — Test (warm-up)\")\n",
    "\n",
    "print(\"Saved PR curves (if available) to artifacts/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8941fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote artifacts/env_snapshot.json\n",
      "{'python': '3.10.11', 'platform': 'win32', 'packages': {'tensorflow': '2.10.1', 'numpy': '1.23.5', 'pandas': '2.3.2', 'scikit-learn': '1.7.2', 'PyYAML': '6.0.2', 'matplotlib': '3.10.6'}}\n"
     ]
    }
   ],
   "source": [
    "ART = Path(\"artifacts\")\n",
    "env = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": sys.platform,\n",
    "    \"packages\": {}\n",
    "}\n",
    "\n",
    "for pkg, imp in {\n",
    "    \"tensorflow\": \"tensorflow\",\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"scikit-learn\": \"sklearn\",\n",
    "    \"PyYAML\": \"yaml\",\n",
    "    \"matplotlib\": \"matplotlib\"\n",
    "}.items():\n",
    "    try:\n",
    "        mod = __import__(imp)\n",
    "        ver = getattr(mod, \"__version__\", \"unknown\")\n",
    "        env[\"packages\"][pkg] = ver\n",
    "    except Exception:\n",
    "        env[\"packages\"][pkg] = \"not_found\"\n",
    "\n",
    "with open(ART/\"env_snapshot.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"Wrote artifacts/env_snapshot.json\")\n",
    "print(env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
